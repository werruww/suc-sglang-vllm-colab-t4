# -*- coding: utf-8 -*-
"""suc_sglang_vllm_GPU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WvUITBMQ0GU6f6kaGE7WkNPDe2MmbTFM
"""

!pip install git+https://github.com/GeeeekExplorer/nano-vllm.git

!huggingface-cli download --resume-download Qwen/Qwen3-0.6B \
  --local-dir ~/huggingface/Qwen3-0.6B/ \
  --local-dir-use-symlinks False

from nanovllm import LLM, SamplingParams
llm = LLM("/root/huggingface/Qwen3-0.6B", enforce_eager=True, tensor_parallel_size=1)
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM."]
outputs = llm.generate(prompts, sampling_params)
outputs[0]["text"]



import os
os.environ["TORCH_COMPILE_DISABLE"] = "1"

import os
os.environ["TORCH_COMPILE_DISABLE"] = "1"


from nanovllm import LLM, SamplingParams
llm = LLM("/root/huggingface/Qwen3-0.6B", enforce_eager=True, tensor_parallel_size=1)
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM."]
outputs = llm.generate(prompts, sampling_params)
outputs[0]["text"]

!git clone https://github.com/sgl-project/sglang.git

!pip uninstall flash-attn -y
!pip install --no-build-isolation flash-attn

# Commented out IPython magic to ensure Python compatibility.
# %cd sglang

!uv pip install "sglang[all]>=0.4.8"



!python3 -m sglang.launch_server --model-path openai-community/gpt2  --context-length 128

!pip install pyngrok

!pip install pyngrok

2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d
2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d

from pyngrok import ngrok
import threading
import time

ngrok_token = "2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d"
ngrok.set_auth_token(ngrok_token)

# Start ngrok in a separate thread to avoid blocking
def start_ngrok():
    public_url = ngrok.connect(3000).public_url
    print(f"ğŸš€ Ngrok Tunnel Open: {public_url}")

ngrok_thread = threading.Thread(target=start_ngrok)
ngrok_thread.start()

# Wait for ngrok to start (optional)
time.sleep(5)

# Execute your node.js script
!node hello-world.js

!which node
mv aa.js aa.mjs

from pyngrok import ngrok
import threading
import time

ngrok_token = "2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d"
ngrok.set_auth_token(ngrok_token)

# Start ngrok in a separate thread to avoid blocking
def start_ngrok():
    public_url = ngrok.connect(3000).public_url
    print(f"ğŸš€ Ngrok Tunnel Open: {public_url}")

ngrok_thread = threading.Thread(target=start_ngrok)
ngrok_thread.start()

# Wait for ngrok to start (optional)
time.sleep(5)

# Execute your node.js script
!python3 -m sglang.launch_server --model-path openai-community/gpt2  --context-length 128

!curl "http://localhost:30000/v1/chat/completions" \
 -H "Content-Type: application/json" \
 -d '{"temperature": 0, "max_tokens": 100, "model": "deepseek-ai/DeepSeek-V3-0324", "tools": [{"type": "function", "function": {"name": "query_weather", "description": "Get weather of an city, the user should supply a city first", "parameters": {"type": "object", "properties": {"city": {"type": "string", "description": "The city, e.g. Beijing"}}, "required": ["city"]}}}], "messages": [{"role": "user", "content": "Hows the weather like in Qingdao today"}]}'

nohup ollama serve &

!python3 -m sglang.launch_server --model-path openai-community/gpt2  --context-length 128

!nohup python3 -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --context-length 128 > server.log 2>&1 &

!tail server.log

!!pip install "sglang[all]" -q

import sglang as sgl
import time

# Ø§Ù†ØªØ¸Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø®Ø§Ø¯Ù… Ù‚Ø¯ Ø¨Ø¯Ø£ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„
print("Waiting for the server to start...")
time.sleep(6) # Ø§Ù†ØªØ¸Ø± 60 Ø«Ø§Ù†ÙŠØ©

print("Checking server log...")
!tail server.log

# Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø®Ø§Ø¯Ù… Ø§Ù„Ù…Ø­Ù„ÙŠ ÙƒÙˆØ§Ø¬Ù‡Ø© Ø®Ù„ÙÙŠØ© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
# Ø§Ù„Ø±Ø§Ø¨Ø· Ù‡Ùˆ Ù†ÙØ³ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø°ÙŠ ÙŠØ¸Ù‡Ø± ÙÙŠ Ù…Ù„Ù server.log
#sgl.set_default_backend(sgl.OpenAI("http://127.0.0.1:30000/v1"))
sgl.set_default_backend(sgl.OpenAI(api_base="http://127.0.0.1:30000/v1", api_key="EMPTY"))
# Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ
@sgl.function
def text_completion(s):
    s += "Once upon a time, in a land far, far away,"
    s += sgl.gen("story", max_tokens=64)

state = text_completion.run()
print(state["story"])

!curl "http://127.0.0.1:30000/v1/chat/completions" \
-H "Content-Type: application/json" \
-d '{"temperature": 0, "max_tokens": 100, "model": "deepseek-ai/DeepSeek-V3-0324", "tools": [{"type": "function", "function": {"name": "query_weather", "description": "Get weather of an city, the user should supply a city first", "parameters": {"type": "object", "properties": {"city": {"type": "string", "description": "The city, e.g. Beijing"}}, "required": ["city"]}}}], "messages": [{"role": "user", "content": "Hows the weather like in Qingdao today"}]}'

!tail server.log

import sglang as sgl

print("Connecting to the local SGLang server...")

# Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø®Ø§Ø¯Ù… Ø§Ù„Ù…Ø­Ù„ÙŠ Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø±Ø§Ø¨Ø·ØŒ Ù…ÙØªØ§Ø­ ÙˆÙ‡Ù…ÙŠØŒ ÙˆØ§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
sgl.set_default_backend(sgl.OpenAI(
    api_base="http://127.0.0.1:30000/v1",
    api_key="EMPTY",
    model_name="openai-community/gpt2"  # Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ù…Ø¶Ø§Ù
))

# Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ
@sgl.function
def text_completion(s):
    s += "The capital of Egypt is"
    s += sgl.gen("answer", max_tokens=32, temperature=0.7)

print("Running generation...")
state = text_completion.run()

# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©
print("\n--- Generated Text ---")
print(f"The capital of Egypt is{state['answer']}")
print("----------------------")

!ps -ef | grep sglang

import sglang as sgl

print("Connecting to the local SGLang server...")

# Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø®Ø§Ø¯Ù… Ø§Ù„Ù…Ø­Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„ØµØ­ÙŠØ­ base_url
# ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…ÙØªØ§Ø­ ÙˆÙ‡Ù…ÙŠ
sgl.set_default_backend(sgl.OpenAI(
    base_url="http://127.0.0.1:30000/v1",
    api_key="EMPTY",
    model_name="openai-community/gpt2" # Keep this as it was in the user's code, even though we changed the server model, SGLang client uses this name to interact with the backend.
))

# Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©
@sgl.function
def chat_completion(s, user_prompt):
    s += sgl.user(user_prompt)
    s += sgl.assistant(sgl.gen("answer", max_tokens=48, temperature=0.8))


user_prompt = "The weather in Cairo today is"
print("Running generation...")
state = chat_completion.run(user_prompt=user_prompt)


# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©
print("\n--- Generated Text ---")
print(f"{state['answer']}")
print("----------------------")



!ps -ef | grep sglang

!tail server.log

!tail server.log

!pkill -f "python3 -m sglang.launch_server"

meta-llama/Llama-2-7b-chat-hf

!nohup python3 -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --context-length 128 > server.log 2>&1 &

import time
print("Waiting for the server to start...")
time.sleep(10) # Wait for 10 seconds for the server to initialize
print("Done waiting.")

!curl "http://127.0.0.1:30000/v1/chat/completions" \
-H "Content-Type: application/json" \
-d '{"temperature": 0, "max_tokens": 100, "model": "Qwen/Qwen3-0.6B", "messages": [{"role": "user", "content": "Hello"}]}'

!tail server.log

!huggingface-cli login --token xx

import sglang as sgl

print("Connecting to the local SGLang server...")

# Set the local server as the default backend
sgl.set_default_backend(sgl.OpenAI(
    base_url="http://127.0.0.1:30000/v1",
    api_key="EMPTY", # Use an empty key as the local server doesn't require a real key
    model_name="Qwen/Qwen3-0.6B" # Specify the model name being served
))

# Now you can create text using the chat structure
@sgl.function
def chat_completion(s, user_prompt):
    s += sgl.user(user_prompt)
    s += sgl.assistant(sgl.gen("answer", max_tokens=48, temperature=0.8))


user_prompt = "The weather in Cairo today is"
print("Running generation...")
state = chat_completion.run(user_prompt=user_prompt)


# Print the result
print("\n--- Generated Text ---")
print(f"{state['answer']}")
print("----------------------")

!pkill -f "python3 -m sglang.launch_server"

!nohup python3 -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --context-length 128 > server.log 2>&1 &

import time
print("Waiting for the server to start...")
time.sleep(15) # Wait for 15 seconds for the server to initialize, might take longer for the first time model download
print("Done waiting.")

"""
Usage:
python3 local_example_chat.py
"""

import sglang as sgl


@sgl.function
def multi_turn_question(s, question_1, question_2):
    s += sgl.user(question_1)
    s += sgl.assistant(sgl.gen("answer_1", max_tokens=256))
    s += sgl.user(question_2)
    s += sgl.assistant(sgl.gen("answer_2", max_tokens=256))


def single():
    state = multi_turn_question.run(
        question_1="What is the capital of the United States?",
        question_2="List two local attractions.",
    )

    for m in state.messages():
        print(m["role"], ":", m["content"])

    print("\n-- answer_1 --\n", state["answer_1"])


def stream():
    state = multi_turn_question.run(
        question_1="What is the capital of the United States?",
        question_2="List two local attractions.",
        stream=True,
    )

    for out in state.text_iter():
        print(out, end="", flush=True)
    print()


def batch():
    states = multi_turn_question.run_batch(
        [
            {
                "question_1": "What is the capital of the United States?",
                "question_2": "List two local attractions.",
            },
            {
                "question_1": "What is the capital of France?",
                "question_2": "What is the population of this city?",
            },
        ]
    )

    for s in states:
        print(s.messages())


if __name__ == "__main__":
    runtime = sgl.Runtime(model_path="Qwen/Qwen3-0.6B")
    sgl.set_default_backend(runtime)

    # Run a single request
    print("\n========== single ==========\n")
    single()

    # Stream output
    print("\n========== stream ==========\n")
    stream()

    # Run a batch of requests
    print("\n========== batch ==========\n")
    batch()

    runtime.shutdown()

"""### Ø´ØºØ§Ù„"""

!python /content/sglang/examples/frontend_language/quick_start/local_example_chat.py

"""### Ø´ØºØ§Ù„"""

/content/sglang/examples/frontend_language/quick_start/local_example_chat.py



"""
Usage:
python3 local_example_chat.py
"""

import sglang as sgl


@sgl.function
def multi_turn_question(s, question_1, question_2):
    s += sgl.user(question_1)
    s += sgl.assistant(sgl.gen("answer_1", max_tokens=256))
    s += sgl.user(question_2)
    s += sgl.assistant(sgl.gen("answer_2", max_tokens=256))


def single():
    state = multi_turn_question.run(
        question_1="What is the capital of the United States?",
        question_2="List two local attractions.",
    )

    for m in state.messages():
        print(m["role"], ":", m["content"])

    print("\n-- answer_1 --\n", state["answer_1"])


def stream():
    state = multi_turn_question.run(
        question_1="What is the capital of the United States?",
        question_2="List two local attractions.",
        stream=True,
    )

    for out in state.text_iter():
        print(out, end="", flush=True)
    print()


def batch():
    states = multi_turn_question.run_batch(
        [
            {
                "question_1": "What is the capital of the United States?",
                "question_2": "List two local attractions.",
            },
            {
                "question_1": "What is the capital of France?",
                "question_2": "What is the population of this city?",
            },
        ]
    )

    for s in states:
        print(s.messages())


if __name__ == "__main__":
    runtime = sgl.Runtime(model_path="Qwen/Qwen3-0.6B")
    sgl.set_default_backend(runtime)

    # Run a single request
    print("\n========== single ==========\n")
    single()

    # Stream output
    print("\n========== stream ==========\n")
    stream()

    # Run a batch of requests
    print("\n========== batch ==========\n")
    batch()

    runtime.shutdown()

!nohup python3 -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --context-length 128 > server.log 2>&1 &

!tail server.log

!pkill -f "python3 -m sglang.launch_server"

!nohup python3 -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --context-length 128 > server.log 2>&1 &

import time
print("Waiting for the server to start...")
time.sleep(15) # Wait for 15 seconds for the server to initialize, might take longer for the first time model download
print("Done waiting.")

!tail server.log

# Commented out IPython magic to ensure Python compatibility.
# %cd sglang

"""Ø´ØºØ§Ù„"""

!nohup python3 -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --context-length 1024 > server.log 2>&1 &

"""Ø´ØºØ§Ù„"""

!tail -n 20 server.log

!python3 -m sglang.launch_server

"""Ø´ØºØ§Ù„"""

!curl -X POST http://127.0.0.1:30000/v1/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer EMPTY" \
-d '{"model": "Qwen/Qwen3-0.6B","prompt": "Once upon a time, in a kingdom by the sea,","max_tokens": 60,"temperature": 0.8,"stream": false}'

"""### Ø´ØºØ§Ù„"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# curl -X POST http://127.0.0.1:30000/v1/chat/completions \
# -H "Content-Type: application/json" \
# -H "Authorization: Bearer EMPTY" \
# -d '{
#   "model": "Qwen/Qwen3-0.6B",
#   "messages": [
#     {
#       "role": "system",
#       "content": "You are a helpful assistant that provides concise and accurate information."
#     },
#     {
#       "role": "user",
#       "content": "Explain the difference between a list and a tuple in Python."
#     }
#   ],
#   "max_tokens": 150,
#   "temperature": 0.7
# }'
#



!python -m sglang.launch_server --device cpu --model-path Qwen/Qwen3-0.6B \
 --host 0.0.0.0





from transformers import AutoModelForCausalLM, AutoTokenizer
import torch # Ù…Ù† Ø§Ù„Ø¬ÙŠØ¯ Ø§Ø³ØªÙŠØ±Ø§Ø¯ torch Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±

# --- Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…ÙØ­Ù„Ù„ (Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ Ù‚Ø¯Ù…ØªÙ‡) ---
model_name = "Qwen/Qwen3-0.6B"
print("ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙØ­Ù„Ù„ (Tokenizer)...")
tokenizer = AutoTokenizer.from_pretrained(model_name)

print("ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
# Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ¹Ù…Ù„ Ø¹Ù„Ù‰ CPU ÙÙ‚Ø·ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ±Ùƒ Ø§Ù„ÙƒÙˆØ¯ ÙƒÙ…Ø§ Ù‡Ùˆ.
# Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ GPUØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§.
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto", # ÙŠØ³ØªØ®Ø¯Ù… bfloat16 Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø¯Ø¹ÙˆÙ…Ø© Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
    device_map="auto"   # ÙŠÙˆØ²Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ù€ GPU Ø§Ù„Ù…ØªØ§Ø­ Ø£Ùˆ Ø§Ù„Ù€ CPU
)
print("Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„.")

# --- Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙØ¯Ø®Ù„ (Prompt) ---
prompt = "Ù…Ù† Ù‡Ùˆ Ù…ÙƒØªØ´Ù Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© Ø§Ù„Ø£Ø±Ø¶ÙŠØ©ØŸ"

# --- Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„ØªØ±Ù…ÙŠØ² (Tokenization) ---
# Ù†Ø­ÙˆÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… (input IDs) ÙŠÙÙ‡Ù…Ù‡Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
# return_tensors="pt" ØªØ¹Ù†ÙŠ Ø£Ù†Ù†Ø§ Ù†Ø±ÙŠØ¯ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµÙŠØºØ© PyTorch Tensors
inputs = tokenizer(prompt, return_tensors="pt")

# Ù„Ù†Ù‚Ù„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ Ù†ÙØ³ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ø¹Ù„ÙŠÙ‡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ù…Ù‡Ù… Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… GPU)
inputs = inputs.to(model.device)

# --- Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø§Ù„ØªÙˆÙ„ÙŠØ¯ (Generation) ---
print("\n...ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©...")
# Ù†Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© .generate() Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ
# max_new_tokens: ÙŠØ­Ø¯Ø¯ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø² Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡Ø§
# pad_token_id: Ù…Ù‡Ù… Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§ØªØŒ Ù†Ø¶Ø¨Ø·Ù‡ Ø¹Ø§Ø¯Ø©Ù‹ Ø¹Ù„Ù‰ ØªÙˆÙƒÙ† Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¬Ù…Ù„Ø©
outputs = model.generate(
    **inputs,
    max_new_tokens=100,
    pad_token_id=tokenizer.eos_token_id
)

# --- Ø§Ù„Ø®Ø·ÙˆØ© 5: ÙÙƒ Ø§Ù„ØªØ±Ù…ÙŠØ² (Decoding) ---
# Ù†Ø­ÙˆÙ„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù†Ø§ØªØ¬Ø© (output IDs) Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ Ù†Øµ Ù…Ù‚Ø±ÙˆØ¡
# outputs[0] Ù„Ø£Ù† Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ØªÙƒÙˆÙ† Ø¹Ù„Ù‰ Ø´ÙƒÙ„ batchØŒ ÙˆÙ†Ø­Ù† Ù†Ø±ÙŠØ¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰
# skip_special_tokens=True Ù„ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªÙˆÙƒÙ†Ø² Ø§Ù„Ø®Ø§ØµØ© Ù…Ø«Ù„ <|endoftext|>
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# --- Ø§Ù„Ø®Ø·ÙˆØ© 6: Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø© ---
print("\n" + "="*20)
print("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©:")
print(generated_text)
print("="*20)

"""### ayhgØ´ØºØ§Ù„"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch # Ù…Ù† Ø§Ù„Ø¬ÙŠØ¯ Ø§Ø³ØªÙŠØ±Ø§Ø¯ torch Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±

# --- Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…ÙØ­Ù„Ù„ (Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ Ù‚Ø¯Ù…ØªÙ‡) ---
model_name = "Qwen/Qwen3-0.6B"
print("ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙØ­Ù„Ù„ (Tokenizer)...")
tokenizer = AutoTokenizer.from_pretrained(model_name)

print("ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
# Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ¹Ù…Ù„ Ø¹Ù„Ù‰ CPU ÙÙ‚Ø·ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ±Ùƒ Ø§Ù„ÙƒÙˆØ¯ ÙƒÙ…Ø§ Ù‡Ùˆ.
# Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ GPUØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§.
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto", # ÙŠØ³ØªØ®Ø¯Ù… bfloat16 Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø¯Ø¹ÙˆÙ…Ø© Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
    device_map="auto"   # ÙŠÙˆØ²Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ù€ GPU Ø§Ù„Ù…ØªØ§Ø­ Ø£Ùˆ Ø§Ù„Ù€ CPU
)
print("Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„.")

# --- Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙØ¯Ø®Ù„ (Prompt) ---
prompt = "how is ai?"

# --- Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„ØªØ±Ù…ÙŠØ² (Tokenization) ---
# Ù†Ø­ÙˆÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… (input IDs) ÙŠÙÙ‡Ù…Ù‡Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
# return_tensors="pt" ØªØ¹Ù†ÙŠ Ø£Ù†Ù†Ø§ Ù†Ø±ÙŠØ¯ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµÙŠØºØ© PyTorch Tensors
inputs = tokenizer(prompt, return_tensors="pt")

# Ù„Ù†Ù‚Ù„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ Ù†ÙØ³ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ø¹Ù„ÙŠÙ‡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ù…Ù‡Ù… Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… GPU)
inputs = inputs.to(model.device)

# --- Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø§Ù„ØªÙˆÙ„ÙŠØ¯ (Generation) ---
print("\n...ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©...")
# Ù†Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© .generate() Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ
# max_new_tokens: ÙŠØ­Ø¯Ø¯ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø² Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡Ø§
# pad_token_id: Ù…Ù‡Ù… Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§ØªØŒ Ù†Ø¶Ø¨Ø·Ù‡ Ø¹Ø§Ø¯Ø©Ù‹ Ø¹Ù„Ù‰ ØªÙˆÙƒÙ† Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¬Ù…Ù„Ø©
outputs = model.generate(
    **inputs,
    max_new_tokens=100,
    pad_token_id=tokenizer.eos_token_id
)

# --- Ø§Ù„Ø®Ø·ÙˆØ© 5: ÙÙƒ Ø§Ù„ØªØ±Ù…ÙŠØ² (Decoding) ---
# Ù†Ø­ÙˆÙ„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù†Ø§ØªØ¬Ø© (output IDs) Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ Ù†Øµ Ù…Ù‚Ø±ÙˆØ¡
# outputs[0] Ù„Ø£Ù† Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ØªÙƒÙˆÙ† Ø¹Ù„Ù‰ Ø´ÙƒÙ„ batchØŒ ÙˆÙ†Ø­Ù† Ù†Ø±ÙŠØ¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰
# skip_special_tokens=True Ù„ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªÙˆÙƒÙ†Ø² Ø§Ù„Ø®Ø§ØµØ© Ù…Ø«Ù„ <|endoftext|>
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# --- Ø§Ù„Ø®Ø·ÙˆØ© 6: Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø© ---
print("\n" + "="*20)
print("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©:")
print(generated_text)
print("="*20)

!pip install git+https://github.com/GeeeekExplorer/nano-vllm.git

from nanovllm import LLM, SamplingParams

from nanovllm import LLM, SamplingParams
llm = LLM("Qwen/Qwen3-0.6B", enforce_eager=True, tensor_parallel_size=1)
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM."]
outputs = llm.generate(prompts, sampling_params)
outputs[0]["text"]

from nanovllm import LLM, SamplingParams
llm = LLM("Qwen/Qwen3-0.6B")
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM."]
outputs = llm.generate(prompts, sampling_params)
outputs[0]["text"]

!pip install vllm

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

!git clone https://github.com/vllm-project/vllm.git

# --- Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ù…Ø¬Ù„Ø¯ Ù…Ø­Ù„ÙŠ ---
from huggingface_hub import snapshot_download

# Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Hugging Face Hub
model_name = "Qwen/Qwen3-0.6B"

print(f"ÙŠØªÙ… ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_name}...")
# Ø³ØªÙ‚ÙˆÙ… Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¨ØªÙ†Ø²ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ù„ÙŠ Ù„Ù„Ù…Ø¬Ù„Ø¯
model_path = snapshot_download(repo_id=model_name)
print(f"ØªÙ… ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±: {model_path}")

# --- Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ù„ÙŠ Ù…Ø¹ Nano-vLLM ---
from nanovllm import LLM, SamplingParams

# Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…ØªØºÙŠØ± model_path Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙØ¹Ù„ÙŠ
print("\nÙŠØªÙ… ØªÙ‡ÙŠØ¦Ø© Nano-vLLM...")
llm = LLM(model_path)
print("ØªÙ…Øª Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø¨Ù†Ø¬Ø§Ø­.")

# Ø¨Ù‚ÙŠØ© Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ÙƒÙ…Ø§ Ù‡Ùˆ
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM. Can you explain what you are in one sentence?"]

print("\n...ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ...")
outputs = llm.generate(prompts, sampling_params)

# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©
# Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ØªÙƒÙˆÙ† Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³ØŒ ÙƒÙ„ Ù‚Ø§Ù…ÙˆØ³ ÙŠÙ…Ø«Ù„ Ù†ØªÙŠØ¬Ø© Ù„Ù…ÙØ¯Ø®Ù„ ÙˆØ§Ø­Ø¯
# Ù†Ø­Ù† Ù„Ø¯ÙŠÙ†Ø§ Ù…ÙØ¯Ø®Ù„ ÙˆØ§Ø­Ø¯ØŒ Ù„Ø°Ø§ Ù†ØµÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø£ÙˆÙ„ outputs[0]
# Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø®Ù„ Ù…ÙØªØ§Ø­ "text"
generated_text = outputs[0]["text"]

print("\n" + "="*20)
print("Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯:")
print(generated_text)
print("="*20)

import os
import torch
from huggingface_hub import snapshot_download

# --- Ø§Ù„Ø®Ø·ÙˆØ© 0: ØªØ¹ÙŠÙŠÙ† Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© Ù„ØªØ¹Ø·ÙŠÙ„ FlashAttention ---
# ÙŠØ¬Ø¨ Ø£Ù† ÙŠØªÙ… Ù‡Ø°Ø§ Ù‚Ø¨Ù„ Ø§Ø³ØªÙŠØ±Ø§Ø¯ nanovllm Ø£Ùˆ vllm
#os.environ["VLLM_ATTENTION_BACKEND"] = "TORCH"
os.environ["VLLM_ATTENTION_BACKEND"] = "XFORMERS"
# Ø§Ù„Ø¢Ù† Ø§Ø³ØªÙˆØ±Ø¯ Ù…ÙƒØªØ¨Ø© nanovllm
from nanovllm import LLM, SamplingParams

# --- Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙƒÙ…Ø§ ÙØ¹Ù„Ù†Ø§ Ø³Ø§Ø¨Ù‚Ù‹Ø§) ---
model_name = "Qwen/Qwen3-0.6B"
print(f"ÙŠØªÙ… ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_name}...")
model_path = snapshot_download(repo_id=model_name)
print(f"ØªÙ… ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±: {model_path}")

# --- Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªÙ‡ÙŠØ¦Ø© Nano-vLLM ---
print("\nÙŠØªÙ… ØªÙ‡ÙŠØ¦Ø© Nano-vLLM (Ù…Ø¹ TORCH attention backend)...")

# Ø³ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø¢Ù† llm Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø®Ù„ÙÙŠØ© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø©
llm = LLM(model_path)
print("ØªÙ…Øª Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø¨Ù†Ø¬Ø§Ø­.")

# --- Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„ØªÙˆÙ„ÙŠØ¯ ---
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM. Can you explain what you are in one sentence?"]

print("\n...ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ...")
outputs = llm.generate(prompts, sampling_params)

# --- Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø© ---
generated_text = outputs[0]["text"]
print("\n" + "="*20)
print("Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯:")
print(generated_text)
print("="*20)

import torch.distributed as dist

if not dist.is_initialized():
    # Ù‡Ù†Ø§ ØªØ¶Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ ÙŠÙ‚ÙˆÙ… Ø¨Ø§Ù„ØªÙ‡ÙŠØ¦Ø©
    # llm = LLM(...)
    pass
else:
    print("Process group is already initialized. Skipping re-initialization.")
    # Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥ÙŠØ¬Ø§Ø¯ Ø·Ø±ÙŠÙ‚Ø© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒØ§Ø¦Ù† llm Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„
    # Ø£Ùˆ ØªØ¯Ù…ÙŠØ± Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù‚Ø¨Ù„ Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø­Ø¯Ø© Ø¬Ø¯ÙŠØ¯Ø©.

# =========================================================
# Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ ÙÙŠ Ø®Ù„ÙŠØ© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø¹Ø¯ Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø¨ÙŠØ¦Ø© Ø§Ù„Ø¹Ù…Ù„
# =========================================================

import os
import torch
from huggingface_hub import snapshot_download

# --- Ø§Ù„Ø®Ø·ÙˆØ© 0: ØªØ¹ÙŠÙŠÙ† Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© Ù„ØªØ¹Ø·ÙŠÙ„ FlashAttention ---
# ÙŠØ¬Ø¨ Ø£Ù† ÙŠØªÙ… Ù‡Ø°Ø§ Ù‚Ø¨Ù„ Ø§Ø³ØªÙŠØ±Ø§Ø¯ nanovllm
os.environ["VLLM_ATTENTION_BACKEND"] = "TORCH"

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø¨Ø¹Ø¯ ØªØ¹ÙŠÙŠÙ† Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø©
from nanovllm import LLM, SamplingParams

# --- Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ---
model_name = "Qwen/Qwen3-0.6B"
print(f"ÙŠØªÙ… ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_name}...")
model_path = snapshot_download(repo_id=model_name)
print(f"ØªÙ… ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±: {model_path}")

# --- Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªÙ‡ÙŠØ¦Ø© Nano-vLLM ---
print("\nÙŠØªÙ… ØªÙ‡ÙŠØ¦Ø© Nano-vLLM (Ù…Ø¹ TORCH attention backend)...")

# Ù…Ù† Ø§Ù„Ù…ÙØªØ±Ø¶ Ø£Ù† ÙŠØ¹Ù…Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø¢Ù† Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ Ø¬Ù„Ø³Ø© Ù†Ø¸ÙŠÙØ©
llm = LLM(model_path)
print("ØªÙ…Øª Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø¨Ù†Ø¬Ø§Ø­.")

# --- Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„ØªÙˆÙ„ÙŠØ¯ ---
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM. Can you explain what you are in one sentence?"]

print("\n...ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ...")
outputs = llm.generate(prompts, sampling_params)

# --- Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø© ---
generated_text = outputs[0]["text"]
print("\n" + "="*20)
print("Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯:")
print(generated_text)
print("="*20)

# Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ø¢Ù†ØŒ Ø³Ù†Ù‚ÙˆÙ… Ø¨ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø©
import torch
from vllm import LLM, SamplingParams

# Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Hugging Face Hub
model_name = "Qwen/Qwen3-0.6B"

# --- ØªÙ‡ÙŠØ¦Ø© vLLM Ù…Ø¹ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© ---
print("\nÙŠØªÙ… ØªÙ‡ÙŠØ¦Ø© vLLM...")

# vLLM ÙŠÙ…ÙƒÙ†Ù‡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Hugging Face Hub
# Ø£Ù‡Ù… Ø¬Ø²Ø¡ Ù‡Ùˆ attention_backend='torch'
llm = LLM(
    model=model_name,
    tensor_parallel_size=1,    # Ù…Ù‡Ù… Ù„Ù„ØªØ´ØºÙŠÙ„ Ø¹Ù„Ù‰ GPU ÙˆØ§Ø­Ø¯
    attention_backend='torch'  # Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø­Ø§Ø³Ù… Ø§Ù„Ø°ÙŠ ÙŠØ¹Ø·Ù„ FlashAttention
)

print("ØªÙ…Øª Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø¨Ù†Ø¬Ø§Ø­.")

# --- Ø¨Ù‚ÙŠØ© Ø§Ù„ÙƒÙˆØ¯ ÙƒÙ…Ø§ Ù‡Ùˆ ---
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, vLLM. Can you explain what you are in one sentence?"]

print("\n...ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ...")
outputs = llm.generate(prompts, sampling_params)

# --- Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø© ---
# vLLM ÙŠØ±Ø¬Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„ÙƒØ§Ø¦Ù†Ø§ØªØŒ ÙƒÙ„ ÙƒØ§Ø¦Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
# Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯:
generated_text = outputs[0].outputs[0].text

print("\n" + "="*20)
print("Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯:")
print(generated_text)
print("="*20)

from vllm import LLM, SamplingParams

# Sample prompts.
prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
    "The future of AI is",
]
# Create a sampling params object.
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)


def main():
    # Create an LLM.
    llm = LLM(model="facebook/opt-125m")
    # Generate texts from the prompts.
    # The output is a list of RequestOutput objects
    # that contain the prompt, generated text, and other information.
    outputs = llm.generate(prompts, sampling_params)
    # Print the outputs.
    print("\nGenerated Outputs:\n" + "-" * 60)
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"Prompt:    {prompt!r}")
        print(f"Output:    {generated_text!r}")
        print("-" * 60)


if __name__ == "__main__":
    main()

!# Load and run the model:
!vllm serve "facebook/opt-125m"

!

curl -X POST "http://localhost:8000/v1/completions" \
	-H "Content-Type: application/json" \
	--data '{
		"model": "facebook/opt-125m",
		"prompt": "Once upon a time,",
		"max_tokens": 512,
		"temperature": 0.5
	}'

!curl -X POST "http://localhost:8000/v1/completions" \
	-H "Content-Type: application/json" \
	--data '{
		"model": "facebook/opt-125m",
		"prompt": "Once upon a time,",
		"max_tokens": 512,
		"temperature": 0.5
	}'

!nohup vllm serve "facebook/opt-125m" > server.log 2>&1 &

!curl -X POST "http://localhost:8000/v1/completions" \
        -H "Content-Type: application/json" \
        --data '{"model": "facebook/opt-125m","prompt": "Once upon a time,","max_tokens": 512,"temperature": 0.5}'

!curl -X POST "http://localhost:8000/v1/completions" -H "Content-Type: application/json" --data '{"model": "facebook/opt-125m","prompt": "Once upon a time'

!tail server.log

import time
print("Waiting for vLLM server to start...")
time.sleep(10) # Wait for 10 seconds
print("Done waiting.")

!pip uninstall -y xformers

# =========================================================
# Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ - Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„Ù‡ Ø¨Ø¹Ø¯ Ø¥Ù„ØºØ§Ø¡ ØªØ«Ø¨ÙŠØª xformers ÙˆØ¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø¨ÙŠØ¦Ø© Ø§Ù„Ø¹Ù…Ù„
# =========================================================

# Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø¢Ù†ØŒ Ù„Ø£Ù† vLLM Ø³ØªØ®ØªØ§Ø± Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§
# Ø¨Ø¹Ø¯ Ø£Ù† Ø£Ø²Ù„Ù†Ø§ Ø§Ù„Ù…Ø³Ø¨Ø¨Ø§Øª Ù„Ù„Ù…Ø´Ø§ÙƒÙ„.

from vllm import LLM, SamplingParams

# Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model_name = "Qwen/Qwen3-0.6B"
print("ÙŠØªÙ… ØªÙ‡ÙŠØ¦Ø© vLLM...")

# Ø³ØªÙ‚ÙˆÙ… vLLM Ø§Ù„Ø¢Ù† ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨Ø§ÙƒØªØ´Ø§Ù Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ flash-attn Ùˆ xformers
# ÙˆØ³ØªØ¹ÙˆØ¯ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø®Ù„ÙÙŠØ© Ù„Ù€ PyTorch.
llm = LLM(
    model=model_name,
    tensor_parallel_size=1
)

print("ØªÙ…Øª Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø¨Ù†Ø¬Ø§Ø­.")

# Ø§Ù„ØªÙˆÙ„ÙŠØ¯
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, vLLM. Can you explain what you are in one sentence?"]

print("\n...ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ...")
outputs = llm.generate(prompts, sampling_params)

# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©
generated_text = outputs[0].outputs[0].text

print("\n" + "="*20)
print("Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯:")
print(generated_text)
print("="*20)

!nohup vllm serve "facebook/opt-125m" > server.log 2>&1 &

!curl -X POST "http://localhost:8000/v1/completions" -H "Content-Type: application/json" --data '{"model": "facebook/opt-125m","prompt": "Once upon a time"}'

!cat server.log

!curl -X POST "http://localhost:8000/v1/completions" -H "Content-Type: application/json" --data '{"model": "facebook/opt-125m","prompt": "Once upon a time"}'

!pip uninstall -y xformers flash-attn

# =========================================================
# Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ - Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„Ù‡ Ø¨Ø¹Ø¯ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØ¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„Ù‡Ø§
# =========================================================
import os

# Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø­Ø§Ø³Ù…Ø©: ØªØ¹ÙŠÙŠÙ† Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© Ù‚Ø¨Ù„ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø£ÙŠ Ø´ÙŠØ¡ Ù…Ù† vLLM
# Ù‡Ø°Ø§ ÙŠØ®Ø¨Ø± vLLM ØµØ±Ø§Ø­Ø©Ù‹: "Ù„Ø§ ØªØ­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø±ØŒ Ø§Ø°Ù‡Ø¨ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¥Ù„Ù‰ PyTorch".
os.environ["VLLM_ATTENTION_BACKEND"] = "TORCH"

# Ø§Ù„Ø¢Ù† Ù†Ø³ØªÙˆØ±Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
from vllm import LLM, SamplingParams

# Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model_name = "Qwen/Qwen3-0.6B"
print("ÙŠØªÙ… ØªÙ‡ÙŠØ¦Ø© vLLM (Ù…Ø¹ Ø¥Ø¬Ø¨Ø§Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø®Ù„ÙÙŠØ© TORCH)...")

# Ø³ÙŠØªÙ… Ø§Ù„Ø¢Ù† ØªØ¬Ø§Ù‡Ù„ xformers Ùˆ flash-attn ØªÙ…Ø§Ù…Ù‹Ø§
llm = LLM(
    model=model_name,
    tensor_parallel_size=1
)

print("ØªÙ…Øª Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø¨Ù†Ø¬Ø§Ø­.")

# Ø§Ù„ØªÙˆÙ„ÙŠØ¯
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, vLLM. Can you explain what you are in one sentence?"]

print("\n...ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ...")
outputs = llm.generate(prompts, sampling_params)

# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©
generated_text = outputs[0].outputs[0].text

print("\n" + "="*20)
print("Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆÙ„Ø¯:")
print(generated_text)
print("="*20)

!pkill -f vllm

# Ø´ØºÙ„ Ø§Ù„Ø®Ø§Ø¯Ù… ÙÙŠ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ù…Ø¨Ø§Ø´Ø±Ø©
!vllm serve "facebook/opt-125m"

!nohup vllm serve "facebook/opt-125m" > server.log 2>&1 &

!curl -X POST "http://localhost:8000/v1/completions" -H "Content-Type: application/json" --data '{"model": "facebook/opt-125m","prompt": "Once upon a time"}'

!curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer EMPTY" \
-d '{"model": "facebook/opt-125m","prompt": "Once upon a time, in a kingdom by the sea,","max_tokens": 60,"temperature": 0.8,"stream": false}'







# Ø³Ù†Ø³ØªØ®Ø¯Ù… nohup Ùˆ & Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù… ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
# --attention-backend torch Ù‡Ùˆ Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØµØ±ÙŠØ­ Ø§Ù„Ø°ÙŠ Ø³ÙŠØ­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
echo "Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… vLLM ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©..."
nohup vllm serve "Qwen/Qwen3-0.6B" --attention-backend torch > server.log 2>&1 &

# Ø§Ù†ØªØ¸Ø± 30 Ø«Ø§Ù†ÙŠØ© Ù„Ø¥Ø¹Ø·Ø§Ø¡ Ø§Ù„Ø®Ø§Ø¯Ù… ÙˆÙ‚ØªÙ‹Ø§ ÙƒØ§ÙÙŠÙ‹Ø§ Ù„Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„Ø¨Ø¯Ø¡
# Ù‡Ø°Ø§ ÙŠÙ…Ù†Ø¹ Ø®Ø·Ø£ "Connection refused"
echo "Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù…Ø¯Ø© 30 Ø«Ø§Ù†ÙŠØ© Ø­ØªÙ‰ ÙŠØ¨Ø¯Ø£ Ø§Ù„Ø®Ø§Ø¯Ù…..."
sleep 30

echo "Ø§Ù„Ø®Ø§Ø¯Ù… ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¬Ø§Ù‡Ø²Ù‹Ø§ Ø§Ù„Ø¢Ù†. ØªØ­Ù‚Ù‚ Ù…Ù† server.log Ù„Ù„ØªØ£ÙƒÙŠØ¯."
!tail server.log



# Ø³Ù†Ø³ØªØ®Ø¯Ù… nohup Ùˆ & Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù… ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
# --attention-backend torch Ù‡Ùˆ Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØµØ±ÙŠØ­ Ø§Ù„Ø°ÙŠ Ø³ÙŠØ­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
echo "Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… vLLM ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©..."
nohup vllm serve "Qwen/Qwen3-0.6B" --attention-backend torch > server.log 2>&1 &

# Ø§Ù†ØªØ¸Ø± 30 Ø«Ø§Ù†ÙŠØ© Ù„Ø¥Ø¹Ø·Ø§Ø¡ Ø§Ù„Ø®Ø§Ø¯Ù… ÙˆÙ‚ØªÙ‹Ø§ ÙƒØ§ÙÙŠÙ‹Ø§ Ù„Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„Ø¨Ø¯Ø¡
# Ù‡Ø°Ø§ ÙŠÙ…Ù†Ø¹ Ø®Ø·Ø£ "Connection refused"
echo "Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù…Ø¯Ø© 30 Ø«Ø§Ù†ÙŠØ© Ø­ØªÙ‰ ÙŠØ¨Ø¯Ø£ Ø§Ù„Ø®Ø§Ø¯Ù…..."
sleep 30

echo "Ø§Ù„Ø®Ø§Ø¯Ù… ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¬Ø§Ù‡Ø²Ù‹Ø§ Ø§Ù„Ø¢Ù†. ØªØ­Ù‚Ù‚ Ù…Ù† server.log Ù„Ù„ØªØ§ÙƒÙŠØ¯."
tail server.log

# Explicitly set the attention backend to torch
export VLLM_ATTENTION_BACKEND="torch"

# Ø³Ù†Ø³ØªØ®Ø¯Ù… nohup Ùˆ & Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù… ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
# --attention-backend torch Ù‡Ùˆ Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØµØ±ÙŠØ­ Ø§Ù„Ø°ÙŠ Ø³ÙŠØ­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
echo "Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø®Ø§Ø¯Ù… vLLM ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©..."
nohup vllm serve "Qwen/Qwen3-0.6B" --attention-backend torch > server.log 2>&1 &

# Ø§Ù†ØªØ¸Ø± 30 Ø«Ø§Ù†ÙŠØ© Ù„Ø¥Ø¹Ø·Ø§Ø¡ Ø§Ù„Ø®Ø§Ø¯Ù… ÙˆÙ‚ØªÙ‹Ø§ ÙƒØ§ÙÙŠÙ‹Ø§ Ù„Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„Ø¨Ø¯Ø¡
# Ù‡Ø°Ø§ ÙŠÙ…Ù†Ø¹ Ø®Ø·Ø£ "Connection refused"
echo "Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù…Ø¯Ø© 30 Ø«Ø§Ù†ÙŠØ© Ø­ØªÙ‰ ÙŠØ¨Ø¯Ø£ Ø§Ù„Ø®Ø§Ø¯Ù…..."
sleep 30

echo "Ø§Ù„Ø®Ø§Ø¯Ù… ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¬Ø§Ù‡Ø²Ù‹Ø§ Ø§Ù„Ø¢Ù†. ØªØ­Ù‚Ù‚ Ù…Ù† server.log Ù„Ù„ØªØ§ÙƒÙŠØ¯."
tail server.log

# Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù… ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ© ÙˆØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª

!nohup vllm serve "facebook/opt-125m" > server.log 2>&1 &

# Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù…Ø¯Ø© 15 Ø«Ø§Ù†ÙŠØ© Ù„Ø¥Ø¹Ø·Ø§Ø¡ Ø§Ù„Ø®Ø§Ø¯Ù… ÙˆÙ‚ØªÙ‹Ø§ ÙƒØ§ÙÙŠÙ‹Ø§ Ù„Ù„Ø¨Ø¯Ø¡

!sleep 15

# Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¢Ø®Ø± 10 Ø£Ø³Ø·Ø± Ù…Ù† Ù…Ù„Ù Ø§Ù„Ø³Ø¬Ù„ Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø®Ø§Ø¯Ù… ÙŠØ¹Ù…Ù„

!tail server.log

# Ø§Ù„Ø¢Ù† Ù‚Ù… Ø¨Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ø®Ø§Ø¯Ù… Ø§Ù„Ø°ÙŠ ÙŠØ¹Ù…Ù„ Ø¨Ù†Ø¬Ø§Ø­
!curl -X POST "http://localhost:8000/v1/chat/completions" \
-H "Content-Type: application/json" \
--data '{
    "model": "Qwen/Qwen3-0.6B",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant that provides concise and accurate information."
        },
        {
            "role": "user",
            "content": "Explain the difference between a list and a tuple in Python."
        }
    ],
    "max_tokens": 100,
    "temperature": 0.7
}'

!nohup vllm serve "facebook/opt-125m" > server.log 2>&1 &

!pip install xformers

"""Ø§Ù„Ø®Ø·Ø£:

```
ModuleNotFoundError: No module named 'xformers'
```

ÙŠØ¹Ù†ÙŠ Ø£Ù† Ù…ÙƒØªØ¨Ø© `xformers` ØºÙŠØ± Ù…Ø«Ø¨ØªØ©. Ù…ÙƒØªØ¨Ø© `xformers` ØªÙØ³ØªØ®Ø¯Ù… Ù…Ù† Ù‚Ø¨Ù„ Ø¨Ø¹Ø¶ Ù†Ù…Ø§Ø°Ø¬ Hugging Face Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ Ø®ØµÙˆØµØ§Ù‹ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ (generation). Ø¨Ø¹Ø¶ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø«Ù„ `facebook/opt-125m` Ù‚Ø¯ ØªØ­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… `xformers` Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø¯Ø¹ÙˆÙ…Ø©.

---

### âœ… Ø§Ù„Ø­Ù„ Ø§Ù„Ø³Ø±ÙŠØ¹:

Ø«Ø¨Ù‘Øª Ù…ÙƒØªØ¨Ø© `xformers` Ø¨Ø£Ù…Ø± ÙˆØ§Ø­Ø¯:

```bash
pip install xformers
```

> âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø©: ØªØ£ÙƒØ¯ Ø£Ù†Ùƒ ØªØ³ØªØ®Ø¯Ù… **PyTorch 2.0 Ø£Ùˆ Ø£Ø¹Ù„Ù‰**ØŒ ÙˆØ£Ù†Ùƒ Ø¹Ù„Ù‰ **Python 3.8â€“3.11**. `xformers` Ù„Ø§ ÙŠØ¯Ø¹Ù… Python 3.12 Ø¨Ø¹Ø¯.

---

### ğŸ” Ø¥Ø°Ø§ ÙƒÙ†Øª Ø¹Ù„Ù‰ Google Colab:

Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ Ù…Ø¹Ù‹Ø§:

```python
!pip install -U vllm xformers
```

Ø«Ù… Ø£Ø¹Ø¯ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¬Ù„Ø³Ø© (Runtime > Restart Runtime).

---

### âœ… Ø£Ùˆ Ø­Ù„ Ø¨Ø¯ÙŠÙ„ (ØªØ¹Ø·ÙŠÙ„ xformers):

Ø¥Ø°Ø§ ÙƒÙ†Øª Ù„Ø§ ØªØ±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… `xformers`ØŒ Ø£Ø¶Ù Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø± ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª:

```python
import os
os.environ["USE_XFORMERS"] = "0"
```

Ø£Ùˆ Ø¬Ø±Ù‘Ø¨ ØªÙ…Ø±ÙŠØ± `use_xformers=False` (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø¯Ø¹ÙˆÙ…Ù‹Ø§ ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©):

```python
llm = LLM(model="facebook/opt-125m", enforce_eager=True)
```

---

### âœ… ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† PyTorch ÙŠØ¯Ø¹Ù… CUDA:

```python
import torch
print(torch.cuda.is_available())  # ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† True Ø¹Ù„Ù‰ GPU
```

---

Ù‡Ù„ ØªØ¹Ù…Ù„ Ø¹Ù„Ù‰ Google Colab Ø£Ù… Ø¹Ù„Ù‰ Ø¬Ù‡Ø§Ø²Ùƒ Ø§Ù„Ù…Ø­Ù„ÙŠØŸ Ø£Ø³ØªØ·ÙŠØ¹ Ø¥Ø¹Ø·Ø§Ø¤Ùƒ ÙƒÙˆØ¯Ù‹Ø§ Ù…ØªÙƒØ§Ù…Ù„Ù‹Ø§ ÙŠÙ†Ø§Ø³Ø¨ Ø¨ÙŠØ¦ØªÙƒ.

"""

!pip install -U vllm xformers

import os
os.environ["USE_XFORMERS"] = "0"

llm = LLM(model="facebook/opt-125m", enforce_eager=True)

import torch
print(torch.cuda.is_available())  # ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† True Ø¹Ù„Ù‰ GPU

"""### Ø´ØºØ§Ù„"""

from vllm import LLM, SamplingParams

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (vLLM ÙŠØ³ØªØ®Ø¯Ù… PyTorch + HuggingFace)
llm = LLM(model="facebook/opt-125m")

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯
sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=100)

# Ø·Ù„Ø¨ Ø§Ù„Ù†Øµ
prompt = "What is the capital of Egypt?"

# ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
outputs = llm.generate(prompt, sampling_params)

# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©
for output in outputs:
    print(output.outputs[0].text)

"""### Ø´ØºØ§Ù„"""

from vllm import LLM, SamplingParams


prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
    "The future of AI is",
]
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)





llm = LLM(model="facebook/opt-125m")

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")

!vllm serve Qwen/Qwen2.5-1.5B-Instruct

curl http://localhost:8000/v1/models

!python3 -m vllm.entrypoints.api_server --model facebook/opt-125m

!curl http://localhost:8000/generate \
  -d '{"prompt": "What is the capital of France?", "max_tokens": 50}' \
  -H "Content-Type: application/json"

!vllm serve Qwen/Qwen2.5-1.5B-Instruct && curl http://localhost:8000/v1/models

!vllm serve facebook/opt-125m && curl http://localhost:8000/v1/models

!curl http://localhost:8000/v1/models

!curl http://localhost:8000/generate \
  -d '{"prompt": "What is the capital of France?", "max_tokens": 50}' \
  -H "Content-Type: application/json"











"""################################

### Ø´ØºØ§Ù„

Ø´ØºÙ„Øª Ù ØªÙŠØ±Ù…Ù†Ø§Ù„ ÙƒÙˆÙ„Ø§Ø¨
!vllm serve facebook/opt-125m && curl http://localhost:8000/v1/models
Ø¨Ø¹Ø¯ Ù…Ø§ Ø§Ø´ØªØºÙ„ Ø§Ù„Ø³Ø±ÙØ± Ø´ØºÙ„Øª Ø§Ù„Ø®Ù„ÙŠØ© Ø§Ù„Ù„ ØªØ­Øª
"""

!curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer EMPTY" \
-d '{"model": "facebook/opt-125m","prompt": "Once upon a time, in a kingdom by the sea,","max_tokens": 60,"temperature": 0.8,"stream": false}'

"""###############################"""

