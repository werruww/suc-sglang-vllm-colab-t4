# -*- coding: utf-8 -*-
"""suc_sglang_vllm_GPU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WvUITBMQ0GU6f6kaGE7WkNPDe2MmbTFM
"""

!pip install git+https://github.com/GeeeekExplorer/nano-vllm.git

!huggingface-cli download --resume-download Qwen/Qwen3-0.6B \
  --local-dir ~/huggingface/Qwen3-0.6B/ \
  --local-dir-use-symlinks False

from nanovllm import LLM, SamplingParams
llm = LLM("/root/huggingface/Qwen3-0.6B", enforce_eager=True, tensor_parallel_size=1)
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM."]
outputs = llm.generate(prompts, sampling_params)
outputs[0]["text"]



import os
os.environ["TORCH_COMPILE_DISABLE"] = "1"

import os
os.environ["TORCH_COMPILE_DISABLE"] = "1"


from nanovllm import LLM, SamplingParams
llm = LLM("/root/huggingface/Qwen3-0.6B", enforce_eager=True, tensor_parallel_size=1)
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM."]
outputs = llm.generate(prompts, sampling_params)
outputs[0]["text"]

!git clone https://github.com/sgl-project/sglang.git

!pip uninstall flash-attn -y
!pip install --no-build-isolation flash-attn

# Commented out IPython magic to ensure Python compatibility.
# %cd sglang

!uv pip install "sglang[all]>=0.4.8"



!python3 -m sglang.launch_server --model-path openai-community/gpt2  --context-length 128

!pip install pyngrok

!pip install pyngrok

2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d
2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d

from pyngrok import ngrok
import threading
import time

ngrok_token = "2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d"
ngrok.set_auth_token(ngrok_token)

# Start ngrok in a separate thread to avoid blocking
def start_ngrok():
    public_url = ngrok.connect(3000).public_url
    print(f"🚀 Ngrok Tunnel Open: {public_url}")

ngrok_thread = threading.Thread(target=start_ngrok)
ngrok_thread.start()

# Wait for ngrok to start (optional)
time.sleep(5)

# Execute your node.js script
!node hello-world.js

!which node
mv aa.js aa.mjs

from pyngrok import ngrok
import threading
import time

ngrok_token = "2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d"
ngrok.set_auth_token(ngrok_token)

# Start ngrok in a separate thread to avoid blocking
def start_ngrok():
    public_url = ngrok.connect(3000).public_url
    print(f"🚀 Ngrok Tunnel Open: {public_url}")

ngrok_thread = threading.Thread(target=start_ngrok)
ngrok_thread.start()

# Wait for ngrok to start (optional)
time.sleep(5)

# Execute your node.js script
!python3 -m sglang.launch_server --model-path openai-community/gpt2  --context-length 128

!curl "http://localhost:30000/v1/chat/completions" \
 -H "Content-Type: application/json" \
 -d '{"temperature": 0, "max_tokens": 100, "model": "deepseek-ai/DeepSeek-V3-0324", "tools": [{"type": "function", "function": {"name": "query_weather", "description": "Get weather of an city, the user should supply a city first", "parameters": {"type": "object", "properties": {"city": {"type": "string", "description": "The city, e.g. Beijing"}}, "required": ["city"]}}}], "messages": [{"role": "user", "content": "Hows the weather like in Qingdao today"}]}'

nohup ollama serve &

!python3 -m sglang.launch_server --model-path openai-community/gpt2  --context-length 128

!nohup python3 -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --context-length 128 > server.log 2>&1 &

!tail server.log

!!pip install "sglang[all]" -q

import sglang as sgl
import time

# انتظر قليلاً للتأكد من أن الخادم قد بدأ بالكامل
print("Waiting for the server to start...")
time.sleep(6) # انتظر 60 ثانية

print("Checking server log...")
!tail server.log

# قم بتعيين الخادم المحلي كواجهة خلفية افتراضية
# الرابط هو نفس الرابط الذي يظهر في ملف server.log
#sgl.set_default_backend(sgl.OpenAI("http://127.0.0.1:30000/v1"))
sgl.set_default_backend(sgl.OpenAI(api_base="http://127.0.0.1:30000/v1", api_key="EMPTY"))
# الآن يمكنك إنشاء نص
@sgl.function
def text_completion(s):
    s += "Once upon a time, in a land far, far away,"
    s += sgl.gen("story", max_tokens=64)

state = text_completion.run()
print(state["story"])

!curl "http://127.0.0.1:30000/v1/chat/completions" \
-H "Content-Type: application/json" \
-d '{"temperature": 0, "max_tokens": 100, "model": "deepseek-ai/DeepSeek-V3-0324", "tools": [{"type": "function", "function": {"name": "query_weather", "description": "Get weather of an city, the user should supply a city first", "parameters": {"type": "object", "properties": {"city": {"type": "string", "description": "The city, e.g. Beijing"}}, "required": ["city"]}}}], "messages": [{"role": "user", "content": "Hows the weather like in Qingdao today"}]}'

!tail server.log

import sglang as sgl

print("Connecting to the local SGLang server...")

# قم بتعيين الخادم المحلي مع تحديد الرابط، مفتاح وهمي، واسم النموذج
sgl.set_default_backend(sgl.OpenAI(
    api_base="http://127.0.0.1:30000/v1",
    api_key="EMPTY",
    model_name="openai-community/gpt2"  # هذا هو السطر المضاف
))

# الآن يمكنك إنشاء نص
@sgl.function
def text_completion(s):
    s += "The capital of Egypt is"
    s += sgl.gen("answer", max_tokens=32, temperature=0.7)

print("Running generation...")
state = text_completion.run()

# طباعة النتيجة
print("\n--- Generated Text ---")
print(f"The capital of Egypt is{state['answer']}")
print("----------------------")

!ps -ef | grep sglang

import sglang as sgl

print("Connecting to the local SGLang server...")

# قم بتعيين الخادم المحلي باستخدام المعامل الصحيح base_url
# وتحديد اسم النموذج ومفتاح وهمي
sgl.set_default_backend(sgl.OpenAI(
    base_url="http://127.0.0.1:30000/v1",
    api_key="EMPTY",
    model_name="openai-community/gpt2" # Keep this as it was in the user's code, even though we changed the server model, SGLang client uses this name to interact with the backend.
))

# الآن يمكنك إنشاء نص باستخدام هيكل الدردشة
@sgl.function
def chat_completion(s, user_prompt):
    s += sgl.user(user_prompt)
    s += sgl.assistant(sgl.gen("answer", max_tokens=48, temperature=0.8))


user_prompt = "The weather in Cairo today is"
print("Running generation...")
state = chat_completion.run(user_prompt=user_prompt)


# طباعة النتيجة
print("\n--- Generated Text ---")
print(f"{state['answer']}")
print("----------------------")



!ps -ef | grep sglang

!tail server.log

!tail server.log

!pkill -f "python3 -m sglang.launch_server"

meta-llama/Llama-2-7b-chat-hf

!nohup python3 -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --context-length 128 > server.log 2>&1 &

import time
print("Waiting for the server to start...")
time.sleep(10) # Wait for 10 seconds for the server to initialize
print("Done waiting.")

!curl "http://127.0.0.1:30000/v1/chat/completions" \
-H "Content-Type: application/json" \
-d '{"temperature": 0, "max_tokens": 100, "model": "Qwen/Qwen3-0.6B", "messages": [{"role": "user", "content": "Hello"}]}'

!tail server.log

!huggingface-cli login --token xx

import sglang as sgl

print("Connecting to the local SGLang server...")

# Set the local server as the default backend
sgl.set_default_backend(sgl.OpenAI(
    base_url="http://127.0.0.1:30000/v1",
    api_key="EMPTY", # Use an empty key as the local server doesn't require a real key
    model_name="Qwen/Qwen3-0.6B" # Specify the model name being served
))

# Now you can create text using the chat structure
@sgl.function
def chat_completion(s, user_prompt):
    s += sgl.user(user_prompt)
    s += sgl.assistant(sgl.gen("answer", max_tokens=48, temperature=0.8))


user_prompt = "The weather in Cairo today is"
print("Running generation...")
state = chat_completion.run(user_prompt=user_prompt)


# Print the result
print("\n--- Generated Text ---")
print(f"{state['answer']}")
print("----------------------")

!pkill -f "python3 -m sglang.launch_server"

!nohup python3 -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --context-length 128 > server.log 2>&1 &

import time
print("Waiting for the server to start...")
time.sleep(15) # Wait for 15 seconds for the server to initialize, might take longer for the first time model download
print("Done waiting.")

"""
Usage:
python3 local_example_chat.py
"""

import sglang as sgl


@sgl.function
def multi_turn_question(s, question_1, question_2):
    s += sgl.user(question_1)
    s += sgl.assistant(sgl.gen("answer_1", max_tokens=256))
    s += sgl.user(question_2)
    s += sgl.assistant(sgl.gen("answer_2", max_tokens=256))


def single():
    state = multi_turn_question.run(
        question_1="What is the capital of the United States?",
        question_2="List two local attractions.",
    )

    for m in state.messages():
        print(m["role"], ":", m["content"])

    print("\n-- answer_1 --\n", state["answer_1"])


def stream():
    state = multi_turn_question.run(
        question_1="What is the capital of the United States?",
        question_2="List two local attractions.",
        stream=True,
    )

    for out in state.text_iter():
        print(out, end="", flush=True)
    print()


def batch():
    states = multi_turn_question.run_batch(
        [
            {
                "question_1": "What is the capital of the United States?",
                "question_2": "List two local attractions.",
            },
            {
                "question_1": "What is the capital of France?",
                "question_2": "What is the population of this city?",
            },
        ]
    )

    for s in states:
        print(s.messages())


if __name__ == "__main__":
    runtime = sgl.Runtime(model_path="Qwen/Qwen3-0.6B")
    sgl.set_default_backend(runtime)

    # Run a single request
    print("\n========== single ==========\n")
    single()

    # Stream output
    print("\n========== stream ==========\n")
    stream()

    # Run a batch of requests
    print("\n========== batch ==========\n")
    batch()

    runtime.shutdown()

"""### شغال"""

!python /content/sglang/examples/frontend_language/quick_start/local_example_chat.py

"""### شغال"""

/content/sglang/examples/frontend_language/quick_start/local_example_chat.py



"""
Usage:
python3 local_example_chat.py
"""

import sglang as sgl


@sgl.function
def multi_turn_question(s, question_1, question_2):
    s += sgl.user(question_1)
    s += sgl.assistant(sgl.gen("answer_1", max_tokens=256))
    s += sgl.user(question_2)
    s += sgl.assistant(sgl.gen("answer_2", max_tokens=256))


def single():
    state = multi_turn_question.run(
        question_1="What is the capital of the United States?",
        question_2="List two local attractions.",
    )

    for m in state.messages():
        print(m["role"], ":", m["content"])

    print("\n-- answer_1 --\n", state["answer_1"])


def stream():
    state = multi_turn_question.run(
        question_1="What is the capital of the United States?",
        question_2="List two local attractions.",
        stream=True,
    )

    for out in state.text_iter():
        print(out, end="", flush=True)
    print()


def batch():
    states = multi_turn_question.run_batch(
        [
            {
                "question_1": "What is the capital of the United States?",
                "question_2": "List two local attractions.",
            },
            {
                "question_1": "What is the capital of France?",
                "question_2": "What is the population of this city?",
            },
        ]
    )

    for s in states:
        print(s.messages())


if __name__ == "__main__":
    runtime = sgl.Runtime(model_path="Qwen/Qwen3-0.6B")
    sgl.set_default_backend(runtime)

    # Run a single request
    print("\n========== single ==========\n")
    single()

    # Stream output
    print("\n========== stream ==========\n")
    stream()

    # Run a batch of requests
    print("\n========== batch ==========\n")
    batch()

    runtime.shutdown()

!nohup python3 -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --context-length 128 > server.log 2>&1 &

!tail server.log

!pkill -f "python3 -m sglang.launch_server"

!nohup python3 -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --context-length 128 > server.log 2>&1 &

import time
print("Waiting for the server to start...")
time.sleep(15) # Wait for 15 seconds for the server to initialize, might take longer for the first time model download
print("Done waiting.")

!tail server.log

# Commented out IPython magic to ensure Python compatibility.
# %cd sglang

"""شغال"""

!nohup python3 -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --context-length 1024 > server.log 2>&1 &

"""شغال"""

!tail -n 20 server.log

!python3 -m sglang.launch_server

"""شغال"""

!curl -X POST http://127.0.0.1:30000/v1/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer EMPTY" \
-d '{"model": "Qwen/Qwen3-0.6B","prompt": "Once upon a time, in a kingdom by the sea,","max_tokens": 60,"temperature": 0.8,"stream": false}'

"""### شغال"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# curl -X POST http://127.0.0.1:30000/v1/chat/completions \
# -H "Content-Type: application/json" \
# -H "Authorization: Bearer EMPTY" \
# -d '{
#   "model": "Qwen/Qwen3-0.6B",
#   "messages": [
#     {
#       "role": "system",
#       "content": "You are a helpful assistant that provides concise and accurate information."
#     },
#     {
#       "role": "user",
#       "content": "Explain the difference between a list and a tuple in Python."
#     }
#   ],
#   "max_tokens": 150,
#   "temperature": 0.7
# }'
#



!python -m sglang.launch_server --device cpu --model-path Qwen/Qwen3-0.6B \
 --host 0.0.0.0





from transformers import AutoModelForCausalLM, AutoTokenizer
import torch # من الجيد استيراد torch للتحكم في نوع البيانات إذا لزم الأمر

# --- الخطوة 1: تحميل النموذج والمُحلل (الكود الذي قدمته) ---
model_name = "Qwen/Qwen3-0.6B"
print("تحميل المُحلل (Tokenizer)...")
tokenizer = AutoTokenizer.from_pretrained(model_name)

print("تحميل النموذج...")
# إذا كنت تعمل على CPU فقط، يمكنك ترك الكود كما هو.
# إذا كان لديك GPU، سيتم استخدامه تلقائيًا.
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto", # يستخدم bfloat16 إذا كانت مدعومة لتسريع العملية
    device_map="auto"   # يوزع النموذج تلقائيًا على الـ GPU المتاح أو الـ CPU
)
print("اكتمل التحميل.")

# --- الخطوة 2: إعداد المُدخل (Prompt) ---
prompt = "من هو مكتشف الجاذبية الأرضية؟"

# --- الخطوة 3: الترميز (Tokenization) ---
# نحول النص إلى أرقام (input IDs) يفهمها النموذج
# return_tensors="pt" تعني أننا نريد مخرجات بصيغة PyTorch Tensors
inputs = tokenizer(prompt, return_tensors="pt")

# لنقل المدخلات إلى نفس الجهاز الموجود عليه النموذج (مهم عند استخدام GPU)
inputs = inputs.to(model.device)

# --- الخطوة 4: التوليد (Generation) ---
print("\n...يتم توليد الإجابة...")
# نستخدم دالة .generate() لتوليد النص
# max_new_tokens: يحدد الحد الأقصى لعدد التوكنز الجديدة التي سيتم توليدها
# pad_token_id: مهم لتجنب التحذيرات، نضبطه عادةً على توكن نهاية الجملة
outputs = model.generate(
    **inputs,
    max_new_tokens=100,
    pad_token_id=tokenizer.eos_token_id
)

# --- الخطوة 5: فك الترميز (Decoding) ---
# نحول الأرقام الناتجة (output IDs) مرة أخرى إلى نص مقروء
# outputs[0] لأن المخرجات تكون على شكل batch، ونحن نريد النتيجة الأولى
# skip_special_tokens=True لتجاهل التوكنز الخاصة مثل <|endoftext|>
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# --- الخطوة 6: طباعة النتيجة ---
print("\n" + "="*20)
print("الإجابة المولدة:")
print(generated_text)
print("="*20)

"""### ayhgشغال"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch # من الجيد استيراد torch للتحكم في نوع البيانات إذا لزم الأمر

# --- الخطوة 1: تحميل النموذج والمُحلل (الكود الذي قدمته) ---
model_name = "Qwen/Qwen3-0.6B"
print("تحميل المُحلل (Tokenizer)...")
tokenizer = AutoTokenizer.from_pretrained(model_name)

print("تحميل النموذج...")
# إذا كنت تعمل على CPU فقط، يمكنك ترك الكود كما هو.
# إذا كان لديك GPU، سيتم استخدامه تلقائيًا.
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto", # يستخدم bfloat16 إذا كانت مدعومة لتسريع العملية
    device_map="auto"   # يوزع النموذج تلقائيًا على الـ GPU المتاح أو الـ CPU
)
print("اكتمل التحميل.")

# --- الخطوة 2: إعداد المُدخل (Prompt) ---
prompt = "how is ai?"

# --- الخطوة 3: الترميز (Tokenization) ---
# نحول النص إلى أرقام (input IDs) يفهمها النموذج
# return_tensors="pt" تعني أننا نريد مخرجات بصيغة PyTorch Tensors
inputs = tokenizer(prompt, return_tensors="pt")

# لنقل المدخلات إلى نفس الجهاز الموجود عليه النموذج (مهم عند استخدام GPU)
inputs = inputs.to(model.device)

# --- الخطوة 4: التوليد (Generation) ---
print("\n...يتم توليد الإجابة...")
# نستخدم دالة .generate() لتوليد النص
# max_new_tokens: يحدد الحد الأقصى لعدد التوكنز الجديدة التي سيتم توليدها
# pad_token_id: مهم لتجنب التحذيرات، نضبطه عادةً على توكن نهاية الجملة
outputs = model.generate(
    **inputs,
    max_new_tokens=100,
    pad_token_id=tokenizer.eos_token_id
)

# --- الخطوة 5: فك الترميز (Decoding) ---
# نحول الأرقام الناتجة (output IDs) مرة أخرى إلى نص مقروء
# outputs[0] لأن المخرجات تكون على شكل batch، ونحن نريد النتيجة الأولى
# skip_special_tokens=True لتجاهل التوكنز الخاصة مثل <|endoftext|>
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# --- الخطوة 6: طباعة النتيجة ---
print("\n" + "="*20)
print("الإجابة المولدة:")
print(generated_text)
print("="*20)

!pip install git+https://github.com/GeeeekExplorer/nano-vllm.git

from nanovllm import LLM, SamplingParams

from nanovllm import LLM, SamplingParams
llm = LLM("Qwen/Qwen3-0.6B", enforce_eager=True, tensor_parallel_size=1)
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM."]
outputs = llm.generate(prompts, sampling_params)
outputs[0]["text"]

from nanovllm import LLM, SamplingParams
llm = LLM("Qwen/Qwen3-0.6B")
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM."]
outputs = llm.generate(prompts, sampling_params)
outputs[0]["text"]

!pip install vllm

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

!git clone https://github.com/vllm-project/vllm.git

# --- الخطوة 1: تنزيل النموذج إلى مجلد محلي ---
from huggingface_hub import snapshot_download

# اسم النموذج على Hugging Face Hub
model_name = "Qwen/Qwen3-0.6B"

print(f"يتم تنزيل النموذج: {model_name}...")
# ستقوم هذه الدالة بتنزيل جميع ملفات النموذج وإرجاع المسار المحلي للمجلد
model_path = snapshot_download(repo_id=model_name)
print(f"تم تنزيل النموذج في المسار: {model_path}")

# --- الخطوة 2: استخدام المسار المحلي مع Nano-vLLM ---
from nanovllm import LLM, SamplingParams

# الآن، قم بتمرير المتغير model_path الذي يحتوي على المسار الفعلي
print("\nيتم تهيئة Nano-vLLM...")
llm = LLM(model_path)
print("تمت التهيئة بنجاح.")

# بقية الكود الخاص بك كما هو
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM. Can you explain what you are in one sentence?"]

print("\n...يتم توليد النص...")
outputs = llm.generate(prompts, sampling_params)

# طباعة النتيجة
# المخرجات تكون قائمة من القواميس، كل قاموس يمثل نتيجة لمُدخل واحد
# نحن لدينا مُدخل واحد، لذا نصل إلى العنصر الأول outputs[0]
# النص المولد موجود داخل مفتاح "text"
generated_text = outputs[0]["text"]

print("\n" + "="*20)
print("النص المولد:")
print(generated_text)
print("="*20)

import os
import torch
from huggingface_hub import snapshot_download

# --- الخطوة 0: تعيين متغير البيئة لتعطيل FlashAttention ---
# يجب أن يتم هذا قبل استيراد nanovllm أو vllm
#os.environ["VLLM_ATTENTION_BACKEND"] = "TORCH"
os.environ["VLLM_ATTENTION_BACKEND"] = "XFORMERS"
# الآن استورد مكتبة nanovllm
from nanovllm import LLM, SamplingParams

# --- الخطوة 1: تنزيل النموذج (كما فعلنا سابقًا) ---
model_name = "Qwen/Qwen3-0.6B"
print(f"يتم تنزيل النموذج: {model_name}...")
model_path = snapshot_download(repo_id=model_name)
print(f"تم تنزيل النموذج في المسار: {model_path}")

# --- الخطوة 2: تهيئة Nano-vLLM ---
print("\nيتم تهيئة Nano-vLLM (مع TORCH attention backend)...")

# سيستخدم الآن llm الواجهة الخلفية المحددة في متغير البيئة
llm = LLM(model_path)
print("تمت التهيئة بنجاح.")

# --- الخطوة 3: التوليد ---
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM. Can you explain what you are in one sentence?"]

print("\n...يتم توليد النص...")
outputs = llm.generate(prompts, sampling_params)

# --- الخطوة 4: طباعة النتيجة ---
generated_text = outputs[0]["text"]
print("\n" + "="*20)
print("النص المولد:")
print(generated_text)
print("="*20)

import torch.distributed as dist

if not dist.is_initialized():
    # هنا تضع الكود الذي يقوم بالتهيئة
    # llm = LLM(...)
    pass
else:
    print("Process group is already initialized. Skipping re-initialization.")
    # قد تحتاج إلى إيجاد طريقة لاستخدام الكائن llm الموجود بالفعل
    # أو تدمير المجموعة الحالية قبل إنشاء واحدة جديدة.

# =========================================================
# قم بتشغيل هذا الكود في خلية واحدة بعد إعادة تشغيل بيئة العمل
# =========================================================

import os
import torch
from huggingface_hub import snapshot_download

# --- الخطوة 0: تعيين متغير البيئة لتعطيل FlashAttention ---
# يجب أن يتم هذا قبل استيراد nanovllm
os.environ["VLLM_ATTENTION_BACKEND"] = "TORCH"

# استيراد المكتبة بعد تعيين متغير البيئة
from nanovllm import LLM, SamplingParams

# --- الخطوة 1: تنزيل النموذج ---
model_name = "Qwen/Qwen3-0.6B"
print(f"يتم تنزيل النموذج: {model_name}...")
model_path = snapshot_download(repo_id=model_name)
print(f"تم تنزيل النموذج في المسار: {model_path}")

# --- الخطوة 2: تهيئة Nano-vLLM ---
print("\nيتم تهيئة Nano-vLLM (مع TORCH attention backend)...")

# من المفترض أن يعمل هذا الآن بنجاح في جلسة نظيفة
llm = LLM(model_path)
print("تمت التهيئة بنجاح.")

# --- الخطوة 3: التوليد ---
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM. Can you explain what you are in one sentence?"]

print("\n...يتم توليد النص...")
outputs = llm.generate(prompts, sampling_params)

# --- الخطوة 4: طباعة النتيجة ---
generated_text = outputs[0]["text"]
print("\n" + "="*20)
print("النص المولد:")
print(generated_text)
print("="*20)

# لا حاجة لمتغير البيئة بعد الآن، سنقوم بتمرير الإعدادات مباشرة
import torch
from vllm import LLM, SamplingParams

# اسم النموذج على Hugging Face Hub
model_name = "Qwen/Qwen3-0.6B"

# --- تهيئة vLLM مع الإعدادات الصحيحة ---
print("\nيتم تهيئة vLLM...")

# vLLM يمكنه تحميل النموذج مباشرة من Hugging Face Hub
# أهم جزء هو attention_backend='torch'
llm = LLM(
    model=model_name,
    tensor_parallel_size=1,    # مهم للتشغيل على GPU واحد
    attention_backend='torch'  # هذا هو السطر الحاسم الذي يعطل FlashAttention
)

print("تمت التهيئة بنجاح.")

# --- بقية الكود كما هو ---
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, vLLM. Can you explain what you are in one sentence?"]

print("\n...يتم توليد النص...")
outputs = llm.generate(prompts, sampling_params)

# --- طباعة النتيجة ---
# vLLM يرجع قائمة من الكائنات، كل كائن يحتوي على قائمة من المخرجات
# للوصول إلى النص المولد:
generated_text = outputs[0].outputs[0].text

print("\n" + "="*20)
print("النص المولد:")
print(generated_text)
print("="*20)

from vllm import LLM, SamplingParams

# Sample prompts.
prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
    "The future of AI is",
]
# Create a sampling params object.
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)


def main():
    # Create an LLM.
    llm = LLM(model="facebook/opt-125m")
    # Generate texts from the prompts.
    # The output is a list of RequestOutput objects
    # that contain the prompt, generated text, and other information.
    outputs = llm.generate(prompts, sampling_params)
    # Print the outputs.
    print("\nGenerated Outputs:\n" + "-" * 60)
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"Prompt:    {prompt!r}")
        print(f"Output:    {generated_text!r}")
        print("-" * 60)


if __name__ == "__main__":
    main()

!# Load and run the model:
!vllm serve "facebook/opt-125m"

!

curl -X POST "http://localhost:8000/v1/completions" \
	-H "Content-Type: application/json" \
	--data '{
		"model": "facebook/opt-125m",
		"prompt": "Once upon a time,",
		"max_tokens": 512,
		"temperature": 0.5
	}'

!curl -X POST "http://localhost:8000/v1/completions" \
	-H "Content-Type: application/json" \
	--data '{
		"model": "facebook/opt-125m",
		"prompt": "Once upon a time,",
		"max_tokens": 512,
		"temperature": 0.5
	}'

!nohup vllm serve "facebook/opt-125m" > server.log 2>&1 &

!curl -X POST "http://localhost:8000/v1/completions" \
        -H "Content-Type: application/json" \
        --data '{"model": "facebook/opt-125m","prompt": "Once upon a time,","max_tokens": 512,"temperature": 0.5}'

!curl -X POST "http://localhost:8000/v1/completions" -H "Content-Type: application/json" --data '{"model": "facebook/opt-125m","prompt": "Once upon a time'

!tail server.log

import time
print("Waiting for vLLM server to start...")
time.sleep(10) # Wait for 10 seconds
print("Done waiting.")

!pip uninstall -y xformers

# =========================================================
# الكود النهائي - قم بتشغيله بعد إلغاء تثبيت xformers وإعادة تشغيل بيئة العمل
# =========================================================

# لا حاجة لمتغير البيئة الآن، لأن vLLM ستختار الواجهة الصحيحة تلقائيًا
# بعد أن أزلنا المسببات للمشاكل.

from vllm import LLM, SamplingParams

# اسم النموذج
model_name = "Qwen/Qwen3-0.6B"
print("يتم تهيئة vLLM...")

# ستقوم vLLM الآن تلقائيًا باكتشاف عدم وجود flash-attn و xformers
# وستعود لاستخدام الواجهة الخلفية لـ PyTorch.
llm = LLM(
    model=model_name,
    tensor_parallel_size=1
)

print("تمت التهيئة بنجاح.")

# التوليد
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, vLLM. Can you explain what you are in one sentence?"]

print("\n...يتم توليد النص...")
outputs = llm.generate(prompts, sampling_params)

# طباعة النتيجة
generated_text = outputs[0].outputs[0].text

print("\n" + "="*20)
print("النص المولد:")
print(generated_text)
print("="*20)

!nohup vllm serve "facebook/opt-125m" > server.log 2>&1 &

!curl -X POST "http://localhost:8000/v1/completions" -H "Content-Type: application/json" --data '{"model": "facebook/opt-125m","prompt": "Once upon a time"}'

!cat server.log

!curl -X POST "http://localhost:8000/v1/completions" -H "Content-Type: application/json" --data '{"model": "facebook/opt-125m","prompt": "Once upon a time"}'

!pip uninstall -y xformers flash-attn

# =========================================================
# الكود النهائي - قم بتشغيله بعد تنظيف البيئة وإعادة تشغيلها
# =========================================================
import os

# الخطوة الحاسمة: تعيين متغير البيئة قبل استيراد أي شيء من vLLM
# هذا يخبر vLLM صراحةً: "لا تحاول استخدام أي شيء آخر، اذهب مباشرة إلى PyTorch".
os.environ["VLLM_ATTENTION_BACKEND"] = "TORCH"

# الآن نستورد المكتبات
from vllm import LLM, SamplingParams

# اسم النموذج
model_name = "Qwen/Qwen3-0.6B"
print("يتم تهيئة vLLM (مع إجبار استخدام الواجهة الخلفية TORCH)...")

# سيتم الآن تجاهل xformers و flash-attn تمامًا
llm = LLM(
    model=model_name,
    tensor_parallel_size=1
)

print("تمت التهيئة بنجاح.")

# التوليد
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, vLLM. Can you explain what you are in one sentence?"]

print("\n...يتم توليد النص...")
outputs = llm.generate(prompts, sampling_params)

# طباعة النتيجة
generated_text = outputs[0].outputs[0].text

print("\n" + "="*20)
print("النص المولد:")
print(generated_text)
print("="*20)

!pkill -f vllm

# شغل الخادم في المقدمة لرؤية الأخطاء مباشرة
!vllm serve "facebook/opt-125m"

!nohup vllm serve "facebook/opt-125m" > server.log 2>&1 &

!curl -X POST "http://localhost:8000/v1/completions" -H "Content-Type: application/json" --data '{"model": "facebook/opt-125m","prompt": "Once upon a time"}'

!curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer EMPTY" \
-d '{"model": "facebook/opt-125m","prompt": "Once upon a time, in a kingdom by the sea,","max_tokens": 60,"temperature": 0.8,"stream": false}'







# سنستخدم nohup و & لتشغيل الخادم في الخلفية
# --attention-backend torch هو الأمر الصريح الذي سيحل المشكلة
echo "بدء تشغيل خادم vLLM في الخلفية..."
nohup vllm serve "Qwen/Qwen3-0.6B" --attention-backend torch > server.log 2>&1 &

# انتظر 30 ثانية لإعطاء الخادم وقتًا كافيًا للتحميل والبدء
# هذا يمنع خطأ "Connection refused"
echo "الانتظار لمدة 30 ثانية حتى يبدأ الخادم..."
sleep 30

echo "الخادم يجب أن يكون جاهزًا الآن. تحقق من server.log للتأكيد."
!tail server.log



# سنستخدم nohup و & لتشغيل الخادم في الخلفية
# --attention-backend torch هو الأمر الصريح الذي سيحل المشكلة
echo "بدء تشغيل خادم vLLM في الخلفية..."
nohup vllm serve "Qwen/Qwen3-0.6B" --attention-backend torch > server.log 2>&1 &

# انتظر 30 ثانية لإعطاء الخادم وقتًا كافيًا للتحميل والبدء
# هذا يمنع خطأ "Connection refused"
echo "الانتظار لمدة 30 ثانية حتى يبدأ الخادم..."
sleep 30

echo "الخادم يجب أن يكون جاهزًا الآن. تحقق من server.log للتاكيد."
tail server.log

# Explicitly set the attention backend to torch
export VLLM_ATTENTION_BACKEND="torch"

# سنستخدم nohup و & لتشغيل الخادم في الخلفية
# --attention-backend torch هو الأمر الصريح الذي سيحل المشكلة
echo "بدء تشغيل خادم vLLM في الخلفية..."
nohup vllm serve "Qwen/Qwen3-0.6B" --attention-backend torch > server.log 2>&1 &

# انتظر 30 ثانية لإعطاء الخادم وقتًا كافيًا للتحميل والبدء
# هذا يمنع خطأ "Connection refused"
echo "الانتظار لمدة 30 ثانية حتى يبدأ الخادم..."
sleep 30

echo "الخادم يجب أن يكون جاهزًا الآن. تحقق من server.log للتاكيد."
tail server.log

# الخطوة 1: بدء تشغيل الخادم في الخلفية وتسجيل المخرجات

!nohup vllm serve "facebook/opt-125m" > server.log 2>&1 &

# الخطوة 2: الانتظار لمدة 15 ثانية لإعطاء الخادم وقتًا كافيًا للبدء

!sleep 15

# الخطوة 3: التحقق من آخر 10 أسطر من ملف السجل للتأكد من أن الخادم يعمل

!tail server.log

# الآن قم بإرسال الطلب إلى الخادم الذي يعمل بنجاح
!curl -X POST "http://localhost:8000/v1/chat/completions" \
-H "Content-Type: application/json" \
--data '{
    "model": "Qwen/Qwen3-0.6B",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant that provides concise and accurate information."
        },
        {
            "role": "user",
            "content": "Explain the difference between a list and a tuple in Python."
        }
    ],
    "max_tokens": 100,
    "temperature": 0.7
}'

!nohup vllm serve "facebook/opt-125m" > server.log 2>&1 &

!pip install xformers

"""الخطأ:

```
ModuleNotFoundError: No module named 'xformers'
```

يعني أن مكتبة `xformers` غير مثبتة. مكتبة `xformers` تُستخدم من قبل بعض نماذج Hugging Face لتحسين السرعة والذاكرة، خصوصاً أثناء التوليد (generation). بعض النماذج مثل `facebook/opt-125m` قد تحاول استخدام `xformers` إذا كانت مدعومة.

---

### ✅ الحل السريع:

ثبّت مكتبة `xformers` بأمر واحد:

```bash
pip install xformers
```

> ⚠️ ملاحظة: تأكد أنك تستخدم **PyTorch 2.0 أو أعلى**، وأنك على **Python 3.8–3.11**. `xformers` لا يدعم Python 3.12 بعد.

---

### 🔁 إذا كنت على Google Colab:

استخدم هذا الكود معًا:

```python
!pip install -U vllm xformers
```

ثم أعد تشغيل الجلسة (Runtime > Restart Runtime).

---

### ✅ أو حل بديل (تعطيل xformers):

إذا كنت لا تريد استخدام `xformers`، أضف هذا السطر في بداية السكريبت:

```python
import os
os.environ["USE_XFORMERS"] = "0"
```

أو جرّب تمرير `use_xformers=False` (إذا كان مدعومًا في الواجهة):

```python
llm = LLM(model="facebook/opt-125m", enforce_eager=True)
```

---

### ✅ تأكد من أن PyTorch يدعم CUDA:

```python
import torch
print(torch.cuda.is_available())  # يجب أن تكون True على GPU
```

---

هل تعمل على Google Colab أم على جهازك المحلي؟ أستطيع إعطاؤك كودًا متكاملًا يناسب بيئتك.

"""

!pip install -U vllm xformers

import os
os.environ["USE_XFORMERS"] = "0"

llm = LLM(model="facebook/opt-125m", enforce_eager=True)

import torch
print(torch.cuda.is_available())  # يجب أن تكون True على GPU

"""### شغال"""

from vllm import LLM, SamplingParams

# تحميل النموذج (vLLM يستخدم PyTorch + HuggingFace)
llm = LLM(model="facebook/opt-125m")

# إعدادات التوليد
sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=100)

# طلب النص
prompt = "What is the capital of Egypt?"

# توليد الرد
outputs = llm.generate(prompt, sampling_params)

# طباعة النتيجة
for output in outputs:
    print(output.outputs[0].text)

"""### شغال"""

from vllm import LLM, SamplingParams


prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
    "The future of AI is",
]
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)





llm = LLM(model="facebook/opt-125m")

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")

!vllm serve Qwen/Qwen2.5-1.5B-Instruct

curl http://localhost:8000/v1/models

!python3 -m vllm.entrypoints.api_server --model facebook/opt-125m

!curl http://localhost:8000/generate \
  -d '{"prompt": "What is the capital of France?", "max_tokens": 50}' \
  -H "Content-Type: application/json"

!vllm serve Qwen/Qwen2.5-1.5B-Instruct && curl http://localhost:8000/v1/models

!vllm serve facebook/opt-125m && curl http://localhost:8000/v1/models

!curl http://localhost:8000/v1/models

!curl http://localhost:8000/generate \
  -d '{"prompt": "What is the capital of France?", "max_tokens": 50}' \
  -H "Content-Type: application/json"











"""################################

### شغال

شغلت ف تيرمنال كولاب
!vllm serve facebook/opt-125m && curl http://localhost:8000/v1/models
بعد ما اشتغل السرفر شغلت الخلية الل تحت
"""

!curl -X POST http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer EMPTY" \
-d '{"model": "facebook/opt-125m","prompt": "Once upon a time, in a kingdom by the sea,","max_tokens": 60,"temperature": 0.8,"stream": false}'

"""###############################"""

